{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Qiran Li - hw4b_release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkhCnnA-32fP"
      },
      "source": [
        "# Problem 1 (40 points)\n",
        "\n",
        "A robot is wandering around a room with some obstacles, labeled as $\\#$ in the grid below. It can occupy any of the free cells labeled with a letter, but we are uncertain about its true location and thus keep a belief distribution over its current location. At each timestep the robot moves from its current cell in one of the four cardinal directions with uniform probability, and moving toward either a wall or a $\\#$ cell will cause the robot to stay in its current cell. For example, starting from A the robot can either stay in A with probability $\\frac12$ or move to B or C with probability $\\frac14$ each.\n",
        "\n",
        "![Picture1.png](data:image/png;base64,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)\n",
        "\n",
        "The robot also makes an observation after each transition, returning what it sees in a randomly chosen cardinal direction. Possibilities include observing $\\#$, \"wall\", or \"empty\" (for a free cell). For example, in C the robot observes \"wall\", $\\#$ (each with probability $\\frac14$), or \"empty\" (with probability $\\frac12$).\n",
        "\n",
        "**Note**: You don't have to show work for solving linear equations or eigenvectors, but please show what equations or matrices you use. Feel free to show your work in Python as well. You may also omit computations that will turn out to be zero based on the provided information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwPiioasIfZ"
      },
      "source": [
        "1.  Suppose that the robot wanders around forever without making observations. What is the stationary distribution over the robot's predicted location? \n",
        "\n",
        "2. The initial distribution $X_0$ is uniform over all possible states. The robot makes a transition and observes $e_1 = \\#$. It then makes a second transition and again observes $e_2 = \\#$. What are the belief distributions $\\Pr(X_1 \\mid e_1)$ and $\\Pr(X_2 \\mid e_1, e_2)$?\n",
        "\n",
        "4. Compute the joint distribution $\\Pr(X_1, X_2 \\mid e_1, e_2)$. Hint: First determine the state sequences with a nonzero probability.\n",
        "\n",
        "5. Are the states $X_1$ and $X_2$ independent given $e_1$ and $e_2$? Why or why not? What is the most likely state sequence(s) of $X_1$ and $X_2$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_c8qZSyuGcJ"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1. \n",
        "Based on the problem, we can construct the Transition Matrix of the Markov Chain. We know we have A, B, C, D, E, F 6 states in this Markov Chain.\n",
        "\n",
        "The Matrix is T, which is displayed in the np array below. \n",
        "\n",
        "\n",
        "[[0.5  0.25 0.25 0.   0.   0.  ]\n",
        " [0.25 0.5  0.   0.25 0.   0.  ]\n",
        " [0.25 0.   0.5  0.25 0.   0.  ]\n",
        " [0.   0.25 0.25 0.   0.25 0.25]\n",
        " [0.   0.   0.   0.25 0.75 0.  ]\n",
        " [0.   0.   0.   0.25 0.   0.75]]\n",
        "\n",
        " let P(A), P(B),...P(F) be the stationary distribution.\n",
        " Here is the linear equations:\n",
        "    1/2p(A) + 1/4p(B) + 1/4p(C) = p(A);\n",
        "    1/4p(A) + 1/2p(B) + 1/4p(D) = p(B);\n",
        "    1/4p(A) + 1/2p(C) + 1/4p(D) = p(C);\n",
        "    1/4p(B) + 1/4p(C) + 1/4p(D) + 1/4p(F) = p(D);\n",
        "    1/4p(D) + 3/4p(E) = p(E)\n",
        "    1/4p(D) + 3/4p(F) = p(F)\n",
        "    p(A) + p(B) + p(C) +p(D) +p(E) + p(F) = 1\n",
        "\n",
        "\n",
        " Solve this gives us:\n",
        "    P(A) = p(B) = ... = p(F) = 1/6\n",
        "\n",
        " Also we can check by times T1 100 times. And the above answer matches. \n",
        "2. \n",
        "The initial distribution X0 is uniform. \n",
        "So\n",
        "        p(X0) = (1/6,1/6,1/6,1/6,1/6,1/6);\n",
        "        e1 = #;\n",
        "T is given above in problem 1; For calculating p(X1|e1):\n",
        "        p(X1) = T dot p(X0) = (1/6,1/6,1/6,1/6,1/6,1/6)ùëá\n",
        "        p(X1|e1) = p(e1|X1)p(X1) / p(e1) \n",
        "        p(e1) = p(#|A)p(A) + p(#|B)p(B) + ... + p(#|F)p(F)\n",
        "              = 0*1/6+1/4*1/6+1/4*1/6+0*1/6+1/2*1/6+1/2*1/6\n",
        "              = 0.25\n",
        "        p(e1|X1)p(X1) = [1/6*0,1/6*1/4,1/4*1/6,0*1/6,1/2*1/6,1/2*1/6]ùëá\n",
        "                      = [0,0.04166667,0.04166667,0,0.08333333,0.08333333]ùëá\n",
        "        p(X1|e1) = p(e1|X1)p(X1) / p(e1) \n",
        "                = [0, 0.16666667, 0.16666667, 0, 0.33333333,\n",
        "       0.33333333]ùëá\n",
        "For calculating p(X2|e1,e2), we can use the Forward algorithm:\n",
        "        f1 = p(X1|e1) = [0, 0.16666667, 0.16666667, 0, 0.33333333,\n",
        "       0.33333333]ùëá\n",
        "        f2 = p(X2|e1,e2) is the value we want to get\n",
        "        f2' = T f1 = [0.08333333, 0.08333333, 0.08333333, 0.25, 0.25,0.25]ùëá\n",
        "        O2 = np.diag([0,1/4,1/4,0,1/2,1/2])\n",
        "        f2 = ‚àù O2f2' = [0, 0.07142857, 0.07142857, 0,0.42857143,0.42857143]ùëá\n",
        "        p(X2|e1,e2) = [0, 0.07142857, 0.07142857, 0,0.42857143,0.42857143]ùëá\n",
        "        \n",
        "3. \n",
        "In order to calculate P(x1,x2|e1,e2) where e1, e2 are both '#', by observing the grid, (x1,x2) could only be (B,B),(C,C),(E,E) and (F,F)\n",
        "\n",
        "        P(x1,x2|e1,e2) = P(x1,x2,e1,e2) / p((e1,e2)\n",
        "                       = P(x1)P(e1|x1)p(x2|x1)p(e2|x2) / p(e1,e2)\n",
        "                       = P(x1)P(e1|x1)p(x2|x1)p(e2|x2) / sum_x1_x2 p(x1,x2,e1,e2)\n",
        "\n",
        "Based on the equation above, we get the probability table as follow:\n",
        "\n",
        "\n",
        "\n",
        "\\begin{array}{c|c|c}\n",
        "\\text{X1}&\\text{X2}&\\text{P}\\\\\n",
        "\\hline\n",
        "B&B&1/14\\\\\n",
        "C&C&1/14\\\\\n",
        "E&E&3/7\\\\\n",
        "F&F&3/7\\\\\n",
        "\\end{array}\n",
        "\n",
        "\n",
        "where, for example, 1/14 is calculated by:\n",
        "$$\\frac{1/6*1/4*1/2*1/4}{1/6*1/4*1/2*1/4 + 1/6*1/4*1/2*1/4 +1/6*1/2*3/4*1/2+1/6*1/2*3/4*1/2}$$ \\\\\n",
        "\n",
        "3/7 is calculated by:\n",
        "$$\\frac{1/6*1/2*3/4*1/2}{1/6*1/4*1/2*1/4 + 1/6*1/4*1/2*1/4 +1/6*1/2*3/4*1/2+1/6*1/2*3/4*1/2}$$\n",
        "\n",
        "other combinations of X1, X2 are all 0.\n",
        "\n",
        "4. \n",
        "\n",
        "X1, X2 are independent given e1, e2. Because P(X1|X2,e1,e2) = P(X1|e1,e2). The most likely state sequence X1, X2 is (E,E) or (F,F)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgTXtGN4Ojtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9820ea0b-6347-429a-b1e1-2c6576ed43e5"
      },
      "source": [
        "# ENTER ANY CODE FOR THE ABOVE PROBLEM HERE\n",
        "import numpy as np\n",
        "import numpy.linalg as nla\n",
        "#1\n",
        "T = np.array([[1/2,1/4,1/4,0,0,0], [1/4,1/2,0,1/4,0,0], [1/4,0,1/2,1/4,0,0], \n",
        "              [0,1/4,1/4,0,1/4,1/4],[0,0,0,1/4,3/4,0],[0,0,0,1/4,0,3/4]])\n",
        "print (T)\n",
        "nla.matrix_power(T,100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5  0.25 0.25 0.   0.   0.  ]\n",
            " [0.25 0.5  0.   0.25 0.   0.  ]\n",
            " [0.25 0.   0.5  0.25 0.   0.  ]\n",
            " [0.   0.25 0.25 0.   0.25 0.25]\n",
            " [0.   0.   0.   0.25 0.75 0.  ]\n",
            " [0.   0.   0.   0.25 0.   0.75]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667],\n",
              "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667],\n",
              "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667],\n",
              "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667],\n",
              "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667],\n",
              "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvR0zbs9pEGX",
        "outputId": "8cfaea2d-93a0-4863-d854-8a4d53a1966f"
      },
      "source": [
        "#2\n",
        "X0 = np.array([1/6,1/6,1/6,1/6,1/6,1/6])\n",
        "X1 = T @ X0\n",
        "X1\n",
        "pe1 = 0*1/6+1/4*1/6+1/4*1/6+0*1/6+1/2*1/6+1/2*1/6\n",
        "pe1\n",
        "p_above = np.array([1/6*0,1/6*1/4,1/4*1/6,0*1/6,1/2*1/6,1/2*1/6])\n",
        "px1_e1 = p_above / pe1\n",
        "print (\"p(X1|e1): \",px1_e1)\n",
        "f1 = px1_e1\n",
        "f2_ = T @ f1\n",
        "O2 = np.diag([0,1/4,1/4,0,1/2,1/2])\n",
        "print (O2 @ f2_)\n",
        "f2 = O2 @ f2_ / np.sum(O2 @ f2_)\n",
        "f2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p(X1|e1):  [0.         0.16666667 0.16666667 0.         0.33333333 0.33333333]\n",
            "[0.         0.02083333 0.02083333 0.         0.125      0.125     ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.07142857, 0.07142857, 0.        , 0.42857143,\n",
              "       0.42857143])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQMAqpGGUu2g"
      },
      "source": [
        "# POS Tagging\n",
        "\n",
        "In this assignment you will explore [part-of-speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging), a standard task in natural language processing. The goal is to identify parts of speech and related labels for each word in a given corpus. HMMs are well suited for this problem, with parts of speech being hidden states and the words themselves being observations.\n",
        "\n",
        "We will be using data from the English EWT treebank from [Universal Dependencies](https://universaldependencies.org/treebanks/en_ewt/index.html), which uses 17 POS tags. We are providing clean versions of training and test data for you. The data format is such that each line contains a word and associated tag, and an empty lines signifies the end of a sentence. Feel free to open the files in a text editor to get an idea.\n",
        "\n",
        "Start by uploading both files to the Jupyter session storage (you should do this each time that you start a new session). Then run the following code cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8YbOgtrrymZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_sentence(f):\n",
        "  sentence = []\n",
        "  while True:\n",
        "    line = f.readline()\n",
        "    if not line or line == '\\n':\n",
        "      return sentence\n",
        "    line = line.strip()\n",
        "    word, tag = line.split(\"\\t\", 1)\n",
        "    sentence.append((word, tag))\n",
        "\n",
        "def read_corpus(file):\n",
        "  f = open(file, 'r', encoding='utf-8')\n",
        "  sentences = []\n",
        "  while True:\n",
        "    sentence = read_sentence(f)\n",
        "    if sentence == []:\n",
        "      return sentences\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4b10sfi4WC"
      },
      "source": [
        "training = read_corpus('train.upos.tsv')\n",
        "TAGS = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', \n",
        "        'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
        "NUM_TAGS = len(TAGS)\n",
        "\n",
        "alpha = 0.1\n",
        "tag_counts = np.zeros(NUM_TAGS)\n",
        "transition_counts = np.zeros((NUM_TAGS,NUM_TAGS))\n",
        "obs_counts = {}\n",
        "\n",
        "for sent in training:\n",
        "  for i in range(len(sent)):\n",
        "    word = sent[i][0]\n",
        "    pos = TAGS.index(sent[i][1])\n",
        "    tag_counts[pos] += 1\n",
        "    if i < len(sent)-1:\n",
        "      transition_counts[TAGS.index(sent[i+1][1]), pos] += 1\n",
        "    if word not in obs_counts:\n",
        "      obs_counts[word] = np.zeros(NUM_TAGS)\n",
        "    (obs_counts[word])[pos] += 1\n",
        "\n",
        "TPROBS = transition_counts / np.sum(transition_counts, axis=0)\n",
        "OPROBS = {'#UNSEEN': alpha*np.ones(NUM_TAGS) / (tag_counts+alpha)}\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts+alpha)"
      ],
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W-eqo75bsxw"
      },
      "source": [
        "The preceding cell estimates the parameters of the HMM model by going through all the training data and counting each tag, word, and transition that appears. ```TPROBS``` is the transition matrix, in the form of a 2d numpy array. ```OPROBS``` is a dictionary of 1d numpy arrays, with keys as the words appearing in the training data and their values being 1d numpy arrays of the emission probabilities. \n",
        "\n",
        "Notice that we're including one extra \"tag\" in ```OPROBS```: an ```#UNSEEN``` tag. This is necessary because we will inevitably encounter words in the test dataset that we have not seen before. For any word that we have not seen, we treat it as if the word is just ```#UNSEEN```. We assign the \"count\" of an ```#UNSEEN``` word to a constant value $\\alpha$, so that it acts as Laplacian smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfThHKofcPQ"
      },
      "source": [
        "## Coding 1 (5 points)\n",
        "\n",
        "Before we build our HMM, let's try a simplistic unigram model: given a new word, assign it the POS tag giving it the highest emission probability. Context of surrounding words is therefore not considered. This can be done by taking the ```argmax``` of the emission probabilities for the given word in OPROBS. Remember that we treat all words that did not appear before as ```#UNSEEN```. Complete the function below to achieve this (make sure to actually return the POS tag itself, not the tag index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZsS0vfoC7t"
      },
      "source": [
        "def unigram(obs):\n",
        "  # Returns the tag of the word obs, as predicted by a unigram model\n",
        "  # YOUR CODE HERE\n",
        "  if obs not in OPROBS.keys():\n",
        "    return TAGS[np.argmax(OPROBS[\"#UNSEEN\"])]\n",
        "  return TAGS[np.argmax(OPROBS[obs])]"
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5IuwxZmhGx_"
      },
      "source": [
        "Test out your unigram model by running the cell below, which will tag the specified data and compute accuracy rates over all words and unseen words only. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_28iq5OhHC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca17a5f-b8cc-457b-8e0d-bcdb3bdf1f98"
      },
      "source": [
        "def evaluate(sentences, method):\n",
        "  correct = 0\n",
        "  correct_unseen = 0\n",
        "  num_words = 0\n",
        "  num_unseen_words = 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = [sent[0] for sent in sentence]\n",
        "    pos = [sent[1] for sent in sentence]\n",
        "    unseen = [word not in OPROBS for word in words]\n",
        "    if method == 'unigram':\n",
        "      predict = [unigram(w) for w in words]\n",
        "    elif method == 'viterbi':\n",
        "      predict = viterbi(words)\n",
        "    else:\n",
        "      print(\"invalid method!\")\n",
        "      return\n",
        "\n",
        "    if len(predict) != len(pos):\n",
        "      print(\"incorrect number of predictions\")\n",
        "      return\n",
        "    correct += sum(1 for i,j in zip(pos, predict) if i==j)\n",
        "    correct_unseen += sum(1 for i,j,k in zip(pos, predict, unseen) if i==j and k)\n",
        "    num_words += len(words)\n",
        "    num_unseen_words += sum(unseen)\n",
        "  \n",
        "  print(\"Accuracy rate on all words: \", correct/num_words)\n",
        "  if num_unseen_words > 0:\n",
        "    print(\"Accuracy rate on unseen words: \", correct_unseen/num_unseen_words)\n",
        "\n",
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'unigram')\n",
        "test = read_corpus('test.upos.tsv')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'unigram')"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9066014359235022\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8124875483125473\n",
            "Accuracy rate on unseen words:  0.003926701570680628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuG-HJE54TfN"
      },
      "source": [
        "## Response 1 (5 points)\n",
        "\n",
        "You should see that accuracy on the training data is about 90\\%. Accuracy on the test data set is lower at about 81\\%, with accuracy on unseen words only being about 0.4\\%. What does this last number mean? Your answer should be something like \"0.4\\% of [GROUP] have a POS tag of [TAG]\". Briefly explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62lnToxylBbn"
      },
      "source": [
        "ENTER YOUR RESPONSE HERE\n",
        "\n",
        "The last number means that 0.4% of Unseen words have POS tags of the correct TAGs that they should have. \n",
        "\n",
        "This 0.4% accuracy tells us that basicly our model has a bad interpretation for unseen words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9mf3c7ymgiN"
      },
      "source": [
        "## Coding 2 (10 points)\n",
        "\n",
        "We will now implement Viterbi to improve our performance over the unigram model. We will split the implementation into several subroutines. First complete the ```elapse_time``` function below. Given a distribution ```m```, it should return an updated \"distribution\" that occurs when applying the Viterbi update in one timestep. In addition, it should also return a list with the *indices* of the most likely prior tag for each current tag.\n",
        "\n",
        "As a hint, these will correspond to ```max``` and ```argmax``` operations, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYnTF9ke87Ku"
      },
      "source": [
        "def elapse_time(m):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  after a single timestep using Viterbi update, along with a list of the \n",
        "  indices of the most likely prior tag for each current tag\n",
        "  \"\"\"\n",
        "  mprime = np.zeros(NUM_TAGS)\n",
        "  prior_tags = np.zeros(NUM_TAGS, dtype=np.int8)\n",
        "  m = m.reshape((len(m),1))\n",
        "  #YOUR CODE HERE\n",
        "  mprime = np.max(TPROBS[:, :, None] * m[None, :, :], axis = 1)\n",
        "  prior_tags = np.argmax(TPROBS[:, :, None] * m[None, :, :], axis = 1)\n",
        "  return mprime.reshape((len(mprime),)), prior_tags.reshape((len(prior_tags),))"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3_tgwvogQoa",
        "outputId": "eeca8103-0334-4d2e-dd72-8b99953e1623"
      },
      "source": [
        "m0 = np.ones(17) / 17\n",
        "mprime, prior_tags = elapse_time(m0)\n",
        "print (mprime)\n",
        "prior_tags"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01300929 0.01230319 0.00834417 0.01870376 0.00654152 0.02120422\n",
            " 0.00157563 0.03494667 0.04146768 0.00677726 0.02840866 0.0164832\n",
            " 0.02241779 0.00253611 0.00291206 0.04178426 0.02093562]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5,  7,  3, 10, 12,  1,  6,  5, 14,  3, 13, 11, 16, 15, 14,  9, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifBYfZpruH2l"
      },
      "source": [
        "You can check your implementation above by finding $m_1'$ given $m_0$ where $m_0$ is a uniform distribution. It should approximately look like\n",
        "```\n",
        "[0.01300929, 0.01230319, 0.00834417, 0.01870376, 0.00654152,\n",
        " 0.02120422, 0.00157563, 0.03494667, 0.04146768, 0.00677726,\n",
        " 0.02840866, 0.0164832 , 0.02241779, 0.00253611, 0.00291206,\n",
        " 0.04178426, 0.02093562]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBgLK4Cpj5w"
      },
      "source": [
        "## Coding 3 (5 points)\n",
        "\n",
        "The second step of the Viterbi algorithm is to reweight probabilities according to observations. Given the \"time elapsed\" message distribution ```mprime``` and an ```obs``` (a word), the function below should return an updated distribution weighted by the emission probabilities for this observation. Remember to account for unseen words by using the ```#UNSEEN``` word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovQm2FmpBw0v"
      },
      "source": [
        "def observe(mprime, obs):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  by weighting mprime by the emission probabilities corresponding to obs\n",
        "  \"\"\"\n",
        "  m = np.zeros(NUM_TAGS)\n",
        "  # YOUR CODE HERE\n",
        "  if obs not in OPROBS.keys():\n",
        "    tags = \"#UNSEEN\"\n",
        "  else:\n",
        "    tags = obs\n",
        "  O = np.diag(OPROBS[tags])\n",
        "  m = O @ mprime\n",
        "  return m"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooo-W61MkvUm",
        "outputId": "749cdaa9-19e7-4cc4-ea65-e222382155f3"
      },
      "source": [
        "m1prime = mprime\n",
        "observe(m1prime,'qwerqrq')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.04248647e-07, 6.97495062e-08, 7.90983910e-08, 1.51409439e-07,\n",
              "       9.75313200e-08, 1.30190262e-07, 2.28982743e-07, 1.00481822e-07,\n",
              "       1.03718457e-06, 1.21737628e-07, 1.52922985e-07, 1.27321753e-07,\n",
              "       9.46733027e-08, 6.60257072e-08, 4.86071760e-07, 1.81102981e-07,\n",
              "       2.47144591e-06])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyn5QcGgvAAh"
      },
      "source": [
        "You can check your implementation above by finding $m_1$ given $m_1'$ (from your ```elapse_time``` test above) and an observation of your choosing. If the observation is an ```#UNSEEN``` word, you should find that $m_1$ looks something like \n",
        "```\n",
        "[1.04248647e-07, 6.97495062e-08, 7.90983910e-08, 1.51409439e-07,\n",
        " 9.75313200e-08, 1.30190262e-07, 2.28982743e-07, 1.00481822e-07,\n",
        " 1.03718457e-06, 1.21737628e-07, 1.52922985e-07, 1.27321753e-07,\n",
        " 9.46733027e-08, 6.60257072e-08, 4.86071760e-07, 1.81102981e-07,\n",
        " 2.47144591e-06]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLewyJPOrhPN"
      },
      "source": [
        "## Coding 4 (20 points)\n",
        "\n",
        "You will now put the two functions you wrote above together into one wrapper ```viterbi``` function. Given a list of word observations, it should return a corresponding list of predicted tags. As you hopefully recall, Viterbi runs in two phases. The \"forward\" phase starts with the ```X0``` distribution and goes through the observations one at a time, modifying the message distribution through time and observation updates for each. You should maintain a growing list of most likely tag pointers that is returned from each elapse time update.\n",
        "\n",
        "The second phase then goes backward. Starting with the tag with the highest likelihood at the end, follow the pointers backward to find the most likely tag for each observation. These tags should then be returned as one overall list of tags when finished (make sure the list of tags is in the right order!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xqPy45-E3hX"
      },
      "source": [
        "def viterbi(observations):\n",
        "  \"\"\"\n",
        "  Given a list of word observations, return a list of predicted tags\n",
        "  \"\"\"\n",
        "  m = np.ones(NUM_TAGS)/NUM_TAGS\n",
        "  # YOUR CODE HERE\n",
        "  # \"Forward\" phase of the Viterbi algorithm\n",
        "  ptags = np.arange(NUM_TAGS)\n",
        "  ptags = ptags.reshape((NUM_TAGS,1))\n",
        "  for i in range(len(observations)):\n",
        "    mprime, prior_tags = elapse_time(m)\n",
        "    m = observe(mprime,observations[i])\n",
        "    ptags = np.hstack((ptags,prior_tags.reshape((NUM_TAGS,1))))\n",
        "  #print (m)\n",
        "  #print (ptags)\n",
        "  # \"Backward\" phase of the Viterbi algorithm\n",
        "  likeli_seq = []\n",
        "  f_index = np.argmax(m)\n",
        "  #print (f_index)\n",
        "  for i in range(len(observations)):\n",
        "    likeli_seq.insert(0,TAGS[f_index])\n",
        "    f_index = ptags[f_index][-i-1]\n",
        "    #print (f_index)\n",
        "  return likeli_seq"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1esjvSf4gqV",
        "outputId": "f79e59af-1cd5-4c8a-ba31-497f967b2323"
      },
      "source": [
        "viterbi(['aafasf'])"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkSnP9YAv5qE"
      },
      "source": [
        "Again, we recommend that you check your implementation with small test cases. For example, running ```viterbi(['#UNSEEN'])``` (or any unseen word in place of ```#UNSEEN```) should return ```['X']```, which means that we are unable to determine the POS tag for a previously unseen word without any context.\n",
        "\n",
        "## Response 2 (5 points)\n",
        "\n",
        "Run your Viterbi implementation on the three phrases below. Explain why the tag prediction is different for the word ```'round'``` in each of them. You can refer to the tag descriptions on UD's [website](https://universaldependencies.org/u/pos/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpCXeCRSxqlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5480484c-368f-43a8-f5f9-663e110bc81d"
      },
      "source": [
        "print(viterbi(['a', 'round', 'circle']))\n",
        "print(viterbi(['play', 'another', 'round']))\n",
        "print(viterbi(['walk', 'round', 'the', 'fence']))"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DET', 'ADJ', 'NOUN']\n",
            "['VERB', 'DET', 'NOUN']\n",
            "['VERB', 'ADP', 'DET', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOdemDNCT7N"
      },
      "source": [
        "ENTER YOUR RESPONSE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsUtrNFwJai"
      },
      "source": [
        "Once you are confident of your implementation, you can run it on the full training and test data sets. You should find that both accuracy rates are better than those of the unigram model, at about 95\\% for the training set and (more importantly) 88\\% for the test set. What's more, the accuracy rate on unseen words should be about 29\\%, a very significant improvement over the unigram predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R30oE7Q9HsiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e6e958-9d2f-4d21-ab31-d7c86d58ce55"
      },
      "source": [
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'viterbi')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9520788633819958\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8766784874686218\n",
            "Accuracy rate on unseen words:  0.293630017452007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcZCyFORyw0y"
      },
      "source": [
        "Viterbi's performance is held back by its accuracy rate on unseen words. One option is to build a higher-order HMM, e.g. a trigram model with transitions determined by the previous two states rather than one state. \n",
        "\n",
        "An alternative fix is to consider the emission probabilities of the ```#UNSEEN``` tag. As you saw in the second coding cell that learns the parameters, the ```#UNSEEN``` tag gets the same count of ```alpha``` for all POS tags. This is equivalent to saying that in a typical corpus, the distribution of unseen words is uniform across all tags. This is not a great assumption; there are probably many more nouns or verbs that we have not encountered than conjunctions or punctuation marks.\n",
        "\n",
        "Instead of a uniform distribution, it may be more reasonable to assume that the distribution of unseen words is similar to that of words that appear only once in the training set. These words are known as [*hapax legomena*](https://en.wikipedia.org/wiki/Hapax_legomenon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4i0FIt92ol"
      },
      "source": [
        "## Coding 5 (5 points)\n",
        "\n",
        "Complete the ```hapax_POS_counts``` function below. Given a dictionary ```obs_counts```, which maps a word to a numpy array counting the number of times it occurred as each POS tag, identify all words with a total count of 1. Add up the POS tag occurrences for each of these words, and return the total POS tag counts over all hapax legomena words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkXdM-ns2gaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a63a89-3aa4-4e20-f15e-980a376dced4"
      },
      "source": [
        "def hapax_POS_counts(obs_counts):\n",
        "  \"\"\"\n",
        "  Given a dictionary of word to count arrays, return the total number \n",
        "  of POS tag appearances only for the words with a total count of 1\n",
        "  \"\"\"\n",
        "  hapax = np.zeros(NUM_TAGS)\n",
        "  # YOUR CODE HERE]\n",
        "  for key,value in obs_counts.items():\n",
        "    if np.sum(value) == 1:\n",
        "      tmp = np.nonzero(value)\n",
        "      hapax[tmp] += 1\n",
        "\n",
        "  return hapax\n",
        "\n",
        "hapax = hapax_POS_counts(obs_counts)\n",
        "print(hapax)"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.060e+03 3.500e+01 3.080e+02 2.000e+01 4.000e+00 1.200e+01 5.400e+01\n",
            " 3.514e+03 6.260e+02 2.000e+00 2.900e+01 2.138e+03 4.000e+01 1.400e+01\n",
            " 3.000e+01 1.645e+03 2.680e+02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSQljsUDPkqz",
        "outputId": "671783f4-4f73-416d-819c-5b686cbfb171"
      },
      "source": [
        "print (TAGS[7], TAGS[11])\n",
        "print (TAGS[9],TAGS[4])"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NOUN PROPN\n",
            "PART CCONJ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y82n2w5cPz8"
      },
      "source": [
        "## Response 3 (5 points)\n",
        "\n",
        "You can check your hapax distribution by verifying that the POS tags of hapax legomena tend to be those associated with a greater diversity of words. Which two POS tags occur most often, and which two POS tags occur least often? Briefly explain, again using some context from the structure of the English language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnoQmowYcSQ0"
      },
      "source": [
        "YOUR RESPONSE HERE\n",
        "NOUN and PROPN POS tags occur most often whereas PART and CCONJ occur least often. NOUN and PROPN occur most often because in English, NOUN and PROPN usually occur only once. For example, London is a PROPN and people would most likely only utilize it once at the front or end of a sentence. \n",
        "\n",
        "\n",
        "For PART, it stands for particle. Particles are function words that must be associated with another word or phrase to impart meaning and that do not satisfy definitions of other universal parts of speech. Examples are \"'s\" or \"not\". Those words usually occur more often. So they have least liklihood to just occur once in a sentence. \n",
        "\n",
        "For CCONJ: coordinating conjunction, examples are \"and\", \"or, \"but\". These words ususually occur more often in sentences since they connect two subsentences together. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtIl-uzzAtnM"
      },
      "source": [
        "\n",
        "Once you've successfully produced a distribution of hapax legomena POS tags, run the cell below. ```OPROBS``` will be recomputed using the new distribution, and Viterbi will be run again on the test data set. The accuracy rate on all words should see a slight tick up, but the accuracy on unseen words should see a marked improvement to about 45\\%. This means that we can correctly tag almost half of words that we have never encountered!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0N1ck989SEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436450e8-cf6f-41a1-81ce-512477b6de4e"
      },
      "source": [
        "hapax = hapax_POS_counts(obs_counts)\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts + hapax)\n",
        "OPROBS['#UNSEEN'] = np.divide(hapax, tag_counts + hapax)\n",
        "\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8938518548033629\n",
            "Accuracy rate on unseen words:  0.4493891797556719\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}